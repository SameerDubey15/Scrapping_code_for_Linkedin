{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e90708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, TimeoutException\n",
    "\n",
    "# Get linkedin username and password form user:\n",
    "print(\"Please enter the exact LinkedIn username you use to login (email/phone?):\")\n",
    "username_string = str(input()) \n",
    "print()\n",
    "print(\"Please enter the exact LinkedIn password:\")\n",
    "password_string = str(input())\n",
    "print()\n",
    "\n",
    "#create a browser-specific (Google Chrome) web navigation simulator:\n",
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the LinkedIn login page#\n",
    "browser.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "# Enter your login credentials\n",
    "elementID = browser.find_element(By.ID,'username')\n",
    "elementID.send_keys(username_string)\n",
    "elementID = browser.find_element(By.ID,'password')\n",
    "elementID.send_keys(password_string)\n",
    "elementID.submit()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcca0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[\"https://www.linkedin.com/company/auxesisgroup/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0be2631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345147c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Title=[]\n",
    "Followers=[]\n",
    "Location=[]\n",
    "URL=[]\n",
    "Phone_no=[]\n",
    "Company_size=[]\n",
    "Domain=[]\n",
    "Founded_year=[]\n",
    "Specility=[]\n",
    "\n",
    "for i in links:\n",
    "    #Open Link\n",
    "    browser.get(i)\n",
    "    random_number = random.randint(2, 4)\n",
    "    time.sleep(random_number)\n",
    "        \n",
    "    #Getting HTMML tags\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "    #Scrapping name\n",
    "    name=browser.find_element(By.TAG_NAME,'h1').text.strip('\\n...')\n",
    "    name\n",
    "    \n",
    "    #Scrapping bio\n",
    "    try:\n",
    "        title = soup.find(\"p\", {'class': 'org-top-card-summary__tagline org-top-card-improvement-summary__tagline'})\n",
    "        bio = title.get_text().strip()\n",
    "    except:\n",
    "        bio='None'\n",
    "    \n",
    "    #Followers\n",
    "    try:\n",
    "        followers = soup.find_all(\"div\", {'class': 'org-top-card-summary-info-list__info-item'})\n",
    "        no_of_fol = followers[2].get_text().strip().replace(\" followers\", \"\")\n",
    "        no_of_fol=int(no_of_fol.replace(\",\", \"\"))\n",
    "    except:\n",
    "        no_of_fol='None'\n",
    "        \n",
    "        \n",
    "    #clicking about section\n",
    "    try:\n",
    "        browser.find_element(By.XPATH,'//h2[@class=\"text-heading-xlarge\"]')\n",
    "    except NoSuchElementException:\n",
    "        about_button=browser.find_element(By.XPATH,'//a[@class=\"ember-view pv3 ph4 t-16 t-bold t-black--light org-page-navigation__item-anchor \"]')\n",
    "        about_button.click()                        \n",
    "        time.sleep(2)  \n",
    "    \n",
    "    pag = browser.page_source\n",
    "    soup2 = BeautifulSoup(pag, 'lxml')\n",
    "    abt_sec=soup2.find('dl',{'class':'overflow-hidden'})\n",
    "    dom=abt_sec.find_all('dd')\n",
    "    dt_tags = abt_sec.find_all('dt',{'class':'mb1 text-heading-medium'})\n",
    "    dd_tags = abt_sec.find_all('dd',{'class':'mb4 t-black--light text-body-medium'})\n",
    "    \n",
    "    #URL of Website\n",
    "    try:\n",
    "        web=abt_sec.find('a',{'class':'link-without-visited-state ember-view'})\n",
    "        Url=web.get_text().strip()\n",
    "    except:\n",
    "        Url=\"None\"\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the extracted information\n",
    "    info_dict = {}\n",
    "    \n",
    "    # Iterate over <dt> tags and extract the information for website,phone, Industry, headquater, company size\n",
    "    i=0\n",
    "    x=1\n",
    "    for dt_tag, dd_tag in zip(dt_tags, dd_tags):\n",
    "        dt_text = dt_tag.text.strip()\n",
    "        if (dt_text== \"Website\" or dt_text== \"Phone\" or dt_text== \"Industry\" or dt_text==\"Headquarters\"):\n",
    "            dd_text = dd_tag.text.strip()\n",
    "        \n",
    "        if dt_text == \"Company size\":\n",
    "            dd_tag = abt_sec.find('dd', {'class': 't-black--light text-body-medium mb1'})\n",
    "            dd_text=dd_tag.get_text().strip().replace(\" employees\", \"\")\n",
    "            \n",
    "        if dt_text == \"Headquarters\":\n",
    "            x=2\n",
    "            dd_tag = dd_tags[i-1]\n",
    "            dd_text=dd_tag.get_text().strip()\n",
    "       \n",
    "        info_dict[dt_text] = dd_text\n",
    "        i=i+1\n",
    "    \n",
    "    #extract the information for founded and specialities\n",
    "    for dt_tag in dt_tags:\n",
    "        dt_text = dt_tag.text.strip()\n",
    "        if dt_text == \"Headquarters\" and x==1:\n",
    "            dd_tag = dd_tags[i-1]\n",
    "            dd_text=dd_tag.get_text().strip()\n",
    "            info_dict[\"Headquarters\"] = dd_text\n",
    "        if dt_text==\"Specialties\":\n",
    "            dd_tag = dd_tags[i-1]\n",
    "            dd_text=dd_tag.get_text().strip()\n",
    "            info_dict[dt_text] = dd_text\n",
    "            dd_tag = dd_tags[i-2]\n",
    "            dd_text=dd_tag.get_text().strip()\n",
    "            info_dict[\"Founded\"] = dd_text\n",
    "        if dt_text==\"Founded\":\n",
    "            dd_tag = dd_tags[i-1]\n",
    "            dd_text=dd_tag.get_text().strip()\n",
    "            info_dict[\"Founded\"] = dd_text\n",
    "            \n",
    "    # Handle case when fields is not present\n",
    "    if \"Specialties\" not in info_dict:\n",
    "        info_dict[\"Specialties\"] = \"None\"\n",
    "    if \"Founded\" not in info_dict:\n",
    "        info_dict[\"Founded\"] = \"None\"\n",
    "    if \"Headquarters\" not in info_dict:\n",
    "        info_dict[\"Headquarters\"] = \"None\"\n",
    "    if \"Website\" not in info_dict:\n",
    "        info_dict[\"Website\"] = \"None\"\n",
    "    if \"Company size\" not in info_dict:\n",
    "        info_dict[\"Company size\"] = \"None\"        \n",
    "    if \"Industry\" not in info_dict:\n",
    "        info_dict[\"Industry\"] = \"None\" \n",
    "        \n",
    "        \n",
    "    #appending for making dataset\n",
    "    Name.append(name)\n",
    "    Title.append(bio)\n",
    "    Followers.append(no_of_fol)\n",
    "    Location.append(info_dict[\"Headquarters\"])\n",
    "    URL.append(info_dict[\"Website\"])\n",
    "    #Phone_no.append(info_dict[\"Phone\"])\n",
    "    Company_size.append(info_dict[\"Company size\"])\n",
    "    Domain.append(info_dict[\"Industry\"])\n",
    "    Founded_year.append(info_dict[\"Founded\"])\n",
    "    Specility.append(info_dict[\"Specialties\"])\n",
    "\n",
    "    del info_dict\n",
    "    random_number = random.randint(1, 10)\n",
    "    time.sleep(random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424770c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Website</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Company size</th>\n",
       "      <th>Location</th>\n",
       "      <th>Founded Year</th>\n",
       "      <th>Specialization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auxesis Group</td>\n",
       "      <td>World's Leading Blockchain Infrastructure Company</td>\n",
       "      <td>3453</td>\n",
       "      <td>https://auxesisgroup.com</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>51-200</td>\n",
       "      <td>Mumbai, Maharashtra</td>\n",
       "      <td>2015</td>\n",
       "      <td>IT Business Consulting, Blockchain Technology,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name                                              Title  \\\n",
       "0  Auxesis Group  World's Leading Blockchain Infrastructure Company   \n",
       "\n",
       "   Followers                   Website                         Domain  \\\n",
       "0       3453  https://auxesisgroup.com  IT Services and IT Consulting   \n",
       "\n",
       "  Company size             Location Founded Year  \\\n",
       "0       51-200  Mumbai, Maharashtra         2015   \n",
       "\n",
       "                                      Specialization  \n",
       "0  IT Business Consulting, Blockchain Technology,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data={'Name':Name,'Title':Title,'Followers':Followers,'Website':URL,'Domain':Domain,'Company size':Company_size,'Location':Location,'Founded Year':Founded_year,'Specialization':Specility}\n",
    "df=pd.DataFrame(data)\n",
    "df.to_excel(\"Ozibook_DBPC.xlsx\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf1bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
